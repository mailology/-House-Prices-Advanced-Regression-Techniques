{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fded91182a90ac599d78abd04706d76b691f3d57"
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e0644cf2d98d2dcf050c4da0741534d8e351774"
      },
      "cell_type": "code",
      "source": "# Save the size of train and test sets\nntrain = train.shape[0]\nntest = test.shape[0]\n\n# Save the 'Id' column of train and test sets\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# Drop the Id column which is irrevelant to our model building\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "732839d019698ee09949419b09858371dedb3bba"
      },
      "cell_type": "code",
      "source": "train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c151a20f7291fb0665430bc3cdb9174dfbbc6d56"
      },
      "cell_type": "code",
      "source": "test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "114fa8d2dc522f863a3cfe019518a2a66935acf4"
      },
      "cell_type": "markdown",
      "source": "## Data Explanation"
    },
    {
      "metadata": {
        "_uuid": "bf883ae8ab04324b60f7dad8b24d0e1c4d477dfe"
      },
      "cell_type": "markdown",
      "source": "**Data fields**\n\nHere's a brief version of what you'll find in the data description file.\n\n1. SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n1. MSSubClass: The building class\n1. MSZoning: The general zoning classification\n1. LotFrontage: Linear feet of street connected to property\n1. LotArea: Lot size in square feet\n1. Street: Type of road access\n1. Alley: Type of alley access\n1. LotShape: General shape of property\n1. LandContour: Flatness of the property\n1. Utilities: Type of utilities available\n1. LotConfig: Lot configuration\n1. LandSlope: Slope of property\n1. Neighborhood: Physical locations within Ames city limits\n1. Condition1: Proximity to main road or railroad\n1. Condition2: Proximity to main road or railroad (if a second is present)\n1. BldgType: Type of dwelling\n1. HouseStyle: Style of dwelling\n1. OverallQual: Overall material and finish quality\n1. OverallCond: Overall condition rating\n1. YearBuilt: Original construction date\n1. YearRemodAdd: Remodel date\n1. RoofStyle: Type of roof\n1. RoofMatl: Roof material\n1. Exterior1st: Exterior covering on house\n1. Exterior2nd: Exterior covering on house (if more than one material)\n1. MasVnrType: Masonry veneer type\n1. MasVnrArea: Masonry veneer area in square feet\n1. ExterQual: Exterior material quality\n1. ExterCond: Present condition of the material on the exterior\n1. Foundation: Type of foundation\n1. BsmtQual: Height of the basement\n1. BsmtCond: General condition of the basement\n1. BsmtExposure: Walkout or garden level basement walls\n1. BsmtFinType1: Quality of basement finished area\n1. BsmtFinSF1: Type 1 finished square feet\n1. BsmtFinType2: Quality of second finished area (if present)\n1. BsmtFinSF2: Type 2 finished square feet\n1. BsmtUnfSF: Unfinished square feet of basement area\n1. TotalBsmtSF: Total square feet of basement area\n1. Heating: Type of heating\n1. HeatingQC: Heating quality and condition\n1. CentralAir: Central air conditioning\n1. Electrical: Electrical system\n1. 1stFlrSF: First Floor square feet\n1. 2ndFlrSF: Second floor square feet\n1. LowQualFinSF: Low quality finished square feet (all floors)\n1. GrLivArea: Above grade (ground) living area square feet\n1. BsmtFullBath: Basement full bathrooms\n1. BsmtHalfBath: Basement half bathrooms\n1. FullBath: Full bathrooms above grade\n1. HalfBath: Half baths above grade\n1. Bedroom: Number of bedrooms above basement level\n1. Kitchen: Number of kitchens\n1. KitchenQual: Kitchen quality\n1. TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n1. Functional: Home functionality rating\n1. Fireplaces: Number of fireplaces\n1. FireplaceQu: Fireplace quality\n1. GarageType: Garage location\n1. GarageYrBlt: Year garage was built\n1. GarageFinish: Interior finish of the garage\n1. GarageCars: Size of garage in car capacity\n1. GarageArea: Size of garage in square feet\n1. GarageQual: Garage quality\n1. GarageCond: Garage condition\n1. PavedDrive: Paved driveway\n1. WoodDeckSF: Wood deck area in square feet\n1. OpenPorchSF: Open porch area in square feet\n1. EnclosedPorch: Enclosed porch area in square feet\n1. 3SsnPorch: Three season porch area in square feet\n1. ScreenPorch: Screen porch area in square feet\n1. PoolArea: Pool area in square feet\n1. PoolQC: Pool quality\n1. Fence: Fence quality\n1. MiscFeature: Miscellaneous feature not covered in other categories\n1. MiscVal: $Value of miscellaneous feature\n1. MoSold: Month Sold\n1. YrSold: Year Sold\n1. SaleType: Type of sale\n1. SaleCondition: Condition of sale\n"
    },
    {
      "metadata": {
        "_uuid": "4007d4080348dbaac799b861a29e43917efeb71f"
      },
      "cell_type": "markdown",
      "source": "## Data Preprocessing\n- identify outliers \n- we are going to impute the missing values by the median (numerical) or mode (categorical) of the respective columns\n- transform categorical variables to dummy variables"
    },
    {
      "metadata": {
        "_uuid": "6e7f460f011297019e50aae3710ac824d7446131"
      },
      "cell_type": "markdown",
      "source": "### 1. Outliers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "496c2dc267909d55f605d4a0ca9cedcbfcafb51c"
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize = 13)\nplt.xlabel('GrLivArea', fontsize = 13)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d39a57da4cd4c9171bffa124dfa21cfd2da05fe"
      },
      "cell_type": "markdown",
      "source": "Note that the houses with more than 4000 square feet from the data set are outliers or unusual observations. It is suggested to remove them (eliminates 5 observations)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5cc447288b54a4d6741f110959b7ce0188255a7"
      },
      "cell_type": "code",
      "source": "# remove outliers and unusual observations\ntrain = train.drop(train[train['GrLivArea'] > 4500].index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "505ab71e312ca8d707719633381d3739e5355fd3"
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize = 13)\nplt.xlabel('GrLivArea', fontsize = 13)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cf259667755d0acb7c5422ba1e0aad23203fd46c"
      },
      "cell_type": "markdown",
      "source": "## Target Variable "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "326db3a2323477f5f760fc7d331c66beac7c57d9"
      },
      "cell_type": "code",
      "source": "from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nsns.distplot(train['SalePrice'] , fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e81a76f39bb7c7ca29e66be1b7ef33ddb8539e7"
      },
      "cell_type": "code",
      "source": "#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8262de2cc52c45674079582f3eadf733cb2a05c0"
      },
      "cell_type": "code",
      "source": "# Update the size of training set\nntrain = train.shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d394b84b31de20d4463bfa83eb048bd02078ee5"
      },
      "cell_type": "code",
      "source": "# Combine training and testing sets into full data\ny_train = train.SalePrice.values\ndata = pd.concat((train, test)).reset_index(drop=True)\ndata.drop(['SalePrice'], axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6bf444defd7c9413a9b78a5d7634f8020b8db070"
      },
      "cell_type": "markdown",
      "source": "### 2. Data Imputation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6578938d77444ae664b6cc9fb7d5b19e8c164992"
      },
      "cell_type": "code",
      "source": "data_na = (data.isnull().sum() / len(data)) * 100\ndata_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :data_na})\nmissing_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c3c51782b080ecdebf4ce7c9f999830783977b8"
      },
      "cell_type": "code",
      "source": "# Number of variables with missing data\nmissing_data.shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66778d3a8e9d8daeb265eff11b726e2ecbaebc67"
      },
      "cell_type": "code",
      "source": "# Variables correlation\nplt.figure(figsize = (18,12))\ncorr = train.corr()\nsns.heatmap(corr, annot=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b661cdc8ddb8b0986e55f413618e3612445c09d1"
      },
      "cell_type": "code",
      "source": "# 'NA' in PoolQC means there is no pool. We can replace it by 'None'\n# 'NA' in MiscFeature means there is no feature, we can replace it by 'None'\n# 'NA' in Alley means there is no alley access for the house, we can replace it by 'None'\n# 'NA' in Fence means there is no fence for the house, we can replace it by 'None'\n# 'NA' in FireplaceQu means there is no Fireplace, we can replace it by 'None'\nfor col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'):\n    data[col] = data[col].fillna('None')\n    \n# LotFrontage:Since the area of each street connected to the house property most likely \n# have a similar area to other houses in its neighborhood , we can fill in missing values \n# by the median LotFrontage of the neighborhood. \ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n# 'NA' in GarageYrBlt implies there is no garage, we can replace it by 0. Similar for GarageArea \n# and GarageCars\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)\n\n# 'NA' in GarageFinish means there is no garage, this makes also the other related variables \n# GarageQual, GarageCond, GarageType becoming 'NA'. Replace all of them by 'None'\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    data[col] = data[col].fillna('None')\n\n# Replace 'NA' in BsmtCond, BsmtExposure, BsmtQual, BsmtFinType2 and BsmtFinType1 by 'None'\nfor col in ('BsmtCond', 'BsmtExposure', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1'):\n    data[col] = data[col].fillna('None')\n\n# Replace 'NA' in MasVnrType by 'None' and replace 'NA' in MasVnrArea by 0\ndata['MasVnrType'] = data['MasVnrType'].fillna('None')\ndata['MasVnrArea'] = data['MasVnrArea'].fillna(0)\n\n# Replace 'NA' in MSZoning by the mode of it, which is 'RL'.\ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\n\n# Replace 'NA' in BsmtFullBath, BsmtHalfBath, TotalBsmtSF, BsmtUnfSF, BsmtFinSF1, BsmtFinSF2 by 0 \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data[col] = data[col].fillna(0)\n\n# Replace 'NA' in Functional by the mode of it, that is 'Typ'\ndata['Functional'] = data['Functional'].fillna(data['Functional'].mode()[0])\n\n# Replace 'NA' in Utilities by the mode of it, that is 'AllPub'\ndata['Utilities'] = data['Utilities'].fillna(data['Utilities'].mode()[0])\n\n# Replace 'NA' in SaleType by the mode of it, that is 'WD'\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])\n\n# Replace 'NA' in KitchenQual by the mode of it, that is 'TA'\ndata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\n\n# Replace 'NA' in Electrical by the mode of it, that is 'TA'\ndata['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])\n\n# Replace 'NA' in Electrical by the mode of it, that is 'TA'\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "548d845c3922997fe8223c04c3e20332a5f9a34c"
      },
      "cell_type": "code",
      "source": "np.sum(data.isnull())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "372a306dd1be36528cdb48a3aece46e0b923498e"
      },
      "cell_type": "markdown",
      "source": "### Incorrect Values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "051111360b266bc112e0c93f9d1ace21084e3987"
      },
      "cell_type": "code",
      "source": "data.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0709202a406a4868f6043c542488b74791bf1d7"
      },
      "cell_type": "code",
      "source": "# The maximum value of GarageYrBlt is 2207 which is obviously wrong. It should be an data\n# input error and the original value should be 2007. \n\ndata[data['GarageYrBlt'] == 2207]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90ff7b933541caa4af3cac31c2e453838acdc2d2"
      },
      "cell_type": "code",
      "source": "data.loc[2590, 'GarageYrBlt'] = 2007",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3228c6b762bec764695f8431c5491e7cdb48164d"
      },
      "cell_type": "code",
      "source": "data.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fc62dcedb1814e1e52f537dcf4c44fc9377f0fdb"
      },
      "cell_type": "markdown",
      "source": "### Factorization "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e1de4585b9158c3f72e1dc282426a016b5e339a"
      },
      "cell_type": "code",
      "source": "# Variable which should be object but it is read as numerical.\nfactors = ['MSSubClass']\n\nfor factor in factors:\n    data.update(data[factor].astype('str'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "df0047c02aae3103e81965c801b931c3404725df"
      },
      "cell_type": "markdown",
      "source": "### Skewed features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "48f1324578a5bb3322534f2136fe293cc64b4ee0"
      },
      "cell_type": "code",
      "source": "numeric_feats = data.dtypes[data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e1e22a60399f5ff45a9664977171a906c12e4001"
      },
      "cell_type": "markdown",
      "source": "### Box-Cox Transformation of highly skewed features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2439f749dd4988c9c5d2f419968ab034da87f56"
      },
      "cell_type": "code",
      "source": "skewness = skewness[abs(skewness.Skew) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    data[feat] = boxcox1p(data[feat], lam)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "26286c0e1714aea6ef26a2415e647bf2bd372b11"
      },
      "cell_type": "markdown",
      "source": "### Check if the level of features are too low which may require deletion"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74a5ab346305a2a6268057bde1515adbb6870930"
      },
      "cell_type": "code",
      "source": "objects = []\nfor i in data.columns:\n    if data[i].dtype == object:\n        objects.append(i)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb5ae8711388cdf01ac185a108115263a5f2fa35"
      },
      "cell_type": "code",
      "source": "sum_features = data[objects].apply(lambda x: len(np.unique(x)))\nsum_features.sort_values(ascending = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "25b22fe58b1e285738a7bad3b6b473ac3601eb22"
      },
      "cell_type": "code",
      "source": "print(data['Utilities'].value_counts())\nprint('------')\nprint(data['Street'].value_counts())\nprint('------')\nprint(data['CentralAir'].value_counts())\nprint('------')\nprint(data['PavedDrive'].value_counts())\nprint('------')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc7ac865eea6ab57f204127697073f55639cacc3"
      },
      "cell_type": "code",
      "source": "# Since Utilites and Street has low amount of level and most of the data are in \n# the same class(>99%), we decide to delete them\n\ndata = data.drop(['Utilities', 'Street'], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e5f2207d0782fd8ae0473a9c98ccbb93c4e7fc7d"
      },
      "cell_type": "markdown",
      "source": "### Create features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "885010f76376329f4845bae862f605f482c92caa"
      },
      "cell_type": "code",
      "source": "data['Total_sqr_footage'] = (data['BsmtFinSF1'] + data['BsmtFinSF2'] +\n                                 data['1stFlrSF'] + data['2ndFlrSF'])\n\ndata['Total_Bathrooms'] = (data['FullBath'] + (0.5*data['HalfBath']) + \n                               data['BsmtFullBath'] + (0.5*data['BsmtHalfBath']))\n\ndata['Total_porch_sf'] = (data['OpenPorchSF'] + data['3SsnPorch'] +\n                              data['EnclosedPorch'] + data['ScreenPorch'] +\n                             data['WoodDeckSF'])\n\n\n#simplified features\ndata['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasgarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasbsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasfireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "439bcdf091fdf7a84f84525390933b45b5b03dcd"
      },
      "cell_type": "markdown",
      "source": "### 3. Transform categorical features to dummy variables"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f025c987b1444792839c8814982a9f0a3ce95a8"
      },
      "cell_type": "code",
      "source": "data.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6731f9b285feb57882833bf01227447ecc55cd29"
      },
      "cell_type": "code",
      "source": "# create dummy variables for categorical features\ndata = pd.get_dummies(data).reset_index(drop=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "acd6a12873325c1fa9f98a35afa0904617a480be"
      },
      "cell_type": "code",
      "source": "# overall data shape\ndata.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5108c2178c8c7341f63fe397f4fa1f299f443357"
      },
      "cell_type": "markdown",
      "source": "## Overfitting preventation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "204148cebb6e8af0141c987de532f6cbfb7361ee"
      },
      "cell_type": "code",
      "source": "X = data.iloc[:ntrain,:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2621c4fcfd2844872f5c6a90194e03c9fdf2147e"
      },
      "cell_type": "code",
      "source": "X['haspool'].value_counts().iloc[0]/len(X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c31bb5f5b1ede8ee3d2470a1b445899a03190b47"
      },
      "cell_type": "code",
      "source": "# To prevent overfitting, dummy columns with more than 97% 1 or 0 will be removed.\n\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(X) * 100 > 99.94:\n        overfit.append(i)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e7e697c5422061f2ead69e19e98c1da33c8c9651"
      },
      "cell_type": "code",
      "source": "overfit",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d5aad7dfb3dfe0ab68befeca45b634ac5bf6001"
      },
      "cell_type": "code",
      "source": "data.drop(overfit, axis=1, inplace=True)\ndata.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0fbf9599ae31637f89dfcb443772fca0078ab501"
      },
      "cell_type": "code",
      "source": "# resplit the data into training and testing sets\ntrain = data[:ntrain]\ntest = data[ntrain:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa629bec088fcbc970ea41f68f38abb458e0748e"
      },
      "cell_type": "markdown",
      "source": "# Building Model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f87730c56c9e5f84662c731b3f28358ebf2eb19"
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "25913ce2ae1f7fc36eda71ce76405a3456151351"
      },
      "cell_type": "code",
      "source": "# Define the covariate matrix for model training and prediction\nX_train = train\nX_test = test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29854c6a09f45526f51a8f22e406f452bc4c86c3"
      },
      "cell_type": "code",
      "source": "#Validation function\nn_folds = 10\n\ndef rmsle_cv(model):\n    kfold = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kfold))\n    return(rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4e416f9d4a8e4abe55aafa4b22d2014033b276fd"
      },
      "cell_type": "markdown",
      "source": "## Lasso regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1bab5a93915cbb24abb7f88c3eac8db26829a852"
      },
      "cell_type": "code",
      "source": "# This model may be very sensitive to outliers. \n# So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=22))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee04c2ba7a929621bd89537be404ba34e1730226"
      },
      "cell_type": "markdown",
      "source": "## Elastic Net Regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8060b5cee38dbc025acdc6feebe35bc2a39594e3"
      },
      "cell_type": "code",
      "source": "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "88a66bf72486376892388125674dbdbf71ae585e"
      },
      "cell_type": "markdown",
      "source": "## Kernel Ridge Regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41c6672a492c9e5f731f302bf5b6710c52a07c4e"
      },
      "cell_type": "code",
      "source": "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "65f8d15813889c5ae13835be4e6cdfe0d5d3ac66"
      },
      "cell_type": "markdown",
      "source": "## Gradient Boosting Regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d0d2a23e2f6262a5268e67e97556468de8e4f25"
      },
      "cell_type": "code",
      "source": "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6dd69b1fa6c7e00cb4eab0eeb5831ec20f1da0ee"
      },
      "cell_type": "markdown",
      "source": "## XGBoost"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6fec07aaaa9e64f527f608ff9115d44fba6af38a"
      },
      "cell_type": "code",
      "source": "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bdd58f78b01d4c1e45173ed1c7d428867b358a43"
      },
      "cell_type": "markdown",
      "source": "## LightGBM"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ba74528883994a17b8468b35765197f893d274e"
      },
      "cell_type": "code",
      "source": "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7fd942a9da952419164fb31fecf35c7a8ab90a99"
      },
      "cell_type": "markdown",
      "source": "## Random Forest Regression model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9c7efeaab9a1db8f877b1f23c9d7d3ddf371f6da",
        "_kg_hide-input": false,
        "_kg_hide-output": false
      },
      "cell_type": "code",
      "source": "# Grid Search\n#from sklearn.ensemble import RandomForestRegressor\n#from sklearn.model_selection import GridSearchCV\n\n#regressor = RandomForestRegressor()\n#parameters = [{'n_estimators' : [100,150,200,250,300], 'max_features' : ['auto','sqrt','log2']}]\n#grid_search = GridSearchCV(estimator = regressor, param_grid = parameters)\n#grid_search = grid_search.fit(X_train, y_train)\n#best_parameters = grid_search.best_params_\n#best_accuracy = grid_search.best_score_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71ea75dd62354c31b1bda62eab9c54b6455dfe2a"
      },
      "cell_type": "code",
      "source": "#best_parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3d2011e53126c3a699d0c7c84b11e9a1cbea08a"
      },
      "cell_type": "code",
      "source": "# Random Forest Regression model\n# Use the best parameters found from above to build the model\n\n#RF = RandomForestRegressor(n_estimators = 300, max_features = 'auto') \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3b6df0c6e7d42908b1cc58ebcde0842a9d10ee77"
      },
      "cell_type": "markdown",
      "source": "## Base Model score"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1790ab6b3de9edc8beb2b4aa91fa297c0500948d"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "002f2314d4e51fb3998e1c9680d72bf6b54f8fb3"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(ENet)\nprint(\"Elastic Net score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b0e5bcebf4278246e7f039571ba3723682ad525"
      },
      "cell_type": "code",
      "source": "#score = rmsle_cv(RF)\n#print(\"Random Forest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "608b84c4cb99b8e4e7419d05e793754e9e6480d8",
        "_kg_hide-output": false
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(GBoost)\nprint(\"Gradient Boost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be5d4318fe0214d8abcaa89164bab578eb6f8929"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(model_xgb)\nprint(\"XGBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98ec61bf45804fa3347054f42780d68bc1c39993"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(model_lgb)\nprint(\"LightGBM score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a58bf66cee6bddc333a0b042b742b080e9a696f7"
      },
      "cell_type": "markdown",
      "source": "## Stacking models"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "683c060751b4ba4fef0d18bbe4a635b4faf9831d"
      },
      "cell_type": "code",
      "source": "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec7ad00a99d851615b28d7c5a3ce70ebdc27c1ce"
      },
      "cell_type": "code",
      "source": "averaged_models = AveragingModels(models = (lasso, ENet, GBoost, model_xgb))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "47f0f51fab2b69a4b309bac94b6f7e074c41b426"
      },
      "cell_type": "markdown",
      "source": "## Less simple Stacking : Adding a Meta-model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e1ff7c38417d65fcf0007bc45130e70cc15a1ce"
      },
      "cell_type": "code",
      "source": "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a0a20da399da1b09a43f2a2cc00941cc375c7bd"
      },
      "cell_type": "code",
      "source": "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, model_xgb),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a11f09be9cf389b4fea95383d6507b8213bf826"
      },
      "cell_type": "code",
      "source": "def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3f3c464c30590f9f7ae90299dc6dd77d1b7bc62a"
      },
      "cell_type": "markdown",
      "source": "### Stacked model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bdb1ad25e6c64042f081e715e4d8774c56a19c3"
      },
      "cell_type": "code",
      "source": "stacked_averaged_models.fit(X_train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(X_train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(X_test.values))\nprint(rmsle(y_train, stacked_train_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3ab3f28fc54cd54b1ee673c356d9a733d34594ac"
      },
      "cell_type": "markdown",
      "source": "### LightGBM "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5aeee7565198bdf8c1d180e5a764044e811f2762"
      },
      "cell_type": "code",
      "source": "model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "887ee60c374eb18b18f9a9d968facc6827f02150"
      },
      "cell_type": "code",
      "source": "print('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 + lgb_train_pred*0.3 ))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "380379bcd0f3c543ac33362e4868353f43057789"
      },
      "cell_type": "markdown",
      "source": "### Ensemble prediction"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "32723048282b876831e014a9f6df2a26fa46cf90"
      },
      "cell_type": "code",
      "source": "ensemble = stacked_pred*0.7 + lgb_pred*0.3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0c56ca23b618eff991d428ba20fe8357c6009b07"
      },
      "cell_type": "markdown",
      "source": "## Submission"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1333958983a319bca46f336cd8684eb54a4bc398"
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79640c046a88edd86c11d46c6da53faf16fe6aab"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}